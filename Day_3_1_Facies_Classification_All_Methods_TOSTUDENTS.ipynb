{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "energy_class",
      "language": "python",
      "name": "energy_class"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QB5kWe7EJ_eH"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cBOat3yKA47"
      },
      "source": [
        "<center><h1>Facies classification using Machine Learning</h1><center>\n",
        "\n",
        "<center>\n",
        "<h2>ICPE 689– Data-Driven Workflows for Intelligent Oil Field Operations and Digital Twins Technology   – Spring 2023</h2>    \n",
        "<h3>Texas A&amp;M University<br>\n",
        "Harold Vance Department of Petroleum Engineering<br><br>\n",
        "Dr. Eduardo Gildin<br>\n",
        "Marcelo Dall'Aqua<br><br>\n",
        " \n",
        "</h3>\n",
        "</center>   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S20GvhUrJ_dj"
      },
      "source": [
        "# Facies classification using Machine Learning\n",
        "\n",
        "#### Created by Dr. Eduardo Gildin and Marcelo Dall'Aqua, Harold Vance Department of Petroleum Engineering, Texas A&M\n",
        "\n",
        "##### Based on the Hall, Brandon - Facies classification using machine learning (2016). Geophysical Tutorial\n",
        "\n",
        "\n",
        "This notebook demonstrates how to train a machine learning algorithm to predict facies from well log data. The dataset we will use comes from a class excercise from The University of Kansas on [Neural Networks and Fuzzy Systems](http://www.people.ku.edu/~gbohling/EECS833/).  This exercise is based on a consortium project to use machine learning techniques to create a reservoir model of the largest gas fields in North America, the Hugoton and Panoma Fields. For more info on the origin of the data, see [Bohling and Dubois (2003)](http://www.kgs.ku.edu/PRS/publication/2003/ofr2003-50.pdf) and [Dubois et al. (2007)](http://dx.doi.org/10.1016/j.cageo.2006.08.011). \n",
        "\n",
        "The dataset we will use is log data from nine wells that have been labeled with a facies type based on oberservation of core.  <font color='red'>We will use this log data to train a several machine learning algorithms to classify facies types. We will use Logistic Regression,  Support vector machines (or SVMs) among others that  are a type of supervised learning model that can be trained on data to perform classification and regression tasks.  The SVM algorithm uses the training data to fit an optimal hyperplane between the different classes (or facies, in our case).  We will use the machine learning  implementation in [scikit-learn](http://scikit-learn.org/stable/modules/svm.html).</font>\n",
        "\n",
        "First we will [explore the dataset](#Exploring-the-dataset).  We will load the training data from 9 wells, and take a look at what we have to work with.  We will plot the data from a couple wells, and create cross plots to look at the variation within the data.  \n",
        "\n",
        "Next we will [condition the data set](#Conditioning-the-data-set).  We will remove the entries that have incomplete data.  The data will be scaled to have zero mean and unit variance.  We will also split the data into training and test sets.\n",
        "\n",
        "We will then be ready to [build the LR classifier].  Later, we will demonstrate how to use the cross validation set to do [model parameter selection](#Model-parameter-selection).\n",
        "\n",
        "Finally, once we have a built and tuned the classifier, we can [apply the trained model](#Applying-the-classification-model-to-new-data) to classify facies in wells which do not already have labels.  We will apply the classifier to two wells, but in principle you could apply the classifier to any number of wells that had the same log data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duHMFrHHJ_dn"
      },
      "source": [
        "##  $\\color{red}{\\text{Exploring the dataset (EDA - Exploratory Data Analysis) }}$\n",
        "\n",
        "First, we will examine the data set we will use to train the classifier.  The training data is contained in the file `facies_vectors.csv`.  The dataset consists of 5 wireline log measurements, two indicator variables and a facies label at half foot intervals.  In machine learning terminology, each log measurement is a **feature vector** that maps a set of **'features'** (the log measurements) to a class (the facies type).  We will use the pandas library to load the data into a dataframe, which provides a convenient data structure to work with well log data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "M1PKmScYJ_do"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "from pandas import set_option\n",
        "set_option(\"display.max_rows\", 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EmSpgHNP8xo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mejUW3GWOsOa"
      },
      "source": [
        "## Eventually wants to save files (picked models) to a Google drive or harddrive --> follows these steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGvVLYVQO027"
      },
      "source": [
        "# First thing, need to  google drive to your Colab session.\n",
        "# You may be asked to sign in or add a code\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/egildin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lDRtdpLPI0i"
      },
      "source": [
        "#Then you can simply write to google drive as you would to a local file system like so:\n",
        "\n",
        "#with open('/content/gdrive/My Drive/file.txt', 'w') as f:\n",
        "#  f.write('content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHizEgkMPh2A"
      },
      "source": [
        "# Now once done, can download the file.. \n",
        "#from google.colab import files\n",
        "#files.download( \"data/dm.ckpt.meta\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m4GgXnHPkLu"
      },
      "source": [
        "## Pickled a model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaFwpCj2Pnwc"
      },
      "source": [
        "import joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwwjEnOKJ_dp"
      },
      "source": [
        "filename = 'facies_training_data.csv'\n",
        "training_data = pd.read_csv(filename)\n",
        "training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFbcYK1OJ_dr"
      },
      "source": [
        "This data is from the Council Grove gas reservoir in Southwest Kansas.  The Panoma Council Grove Field is predominantly a carbonate gas reservoir encompassing 2700 square miles in Southwestern Kansas.  This dataset is from nine wells (with 4149 examples), consisting of a set of seven predictor variables and a rock facies (class) for each example vector and validation (test) data (830 examples from two wells) having the same seven predictor variables in the feature vector.  Facies are based on examination of cores from nine wells taken vertically at half-foot intervals. Predictor variables include five from wireline log measurements and two geologic constraining variables that are derived from geologic knowledge. These are essentially continuous variables sampled at a half-foot sample rate. \n",
        "\n",
        "The seven predictor variables are:\n",
        "* Five wire line log curves include [gamma ray](http://petrowiki.org/Gamma_ray_logs) (GR), [resistivity logging](http://petrowiki.org/Resistivity_and_spontaneous_%28SP%29_logging) (ILD_log10), [neutron-density porosity difference and average neutron-densityporosity](http://petrowiki.org/Neutron_porosity_logs) (DeltaPHI and PHIND), [photoelectric effect](http://www.glossary.oilfield.slb.com/en/Terms/p/photoelectric_effect.aspx) (PE). Note, some wells do not have PE.\n",
        "* Two geologic constraining variables: nonmarine-marine indicator (NM_M) and relative position (RELPOS)\n",
        "\n",
        "The nine discrete facies (classes of rocks) are: \n",
        "1. Nonmarine sandstone - SS\n",
        "2. Nonmarine coarse siltstone - CSiS\n",
        "3. Nonmarine fine siltstone - FSiS\n",
        "4. Marine siltstone and shale - SiSh\n",
        "5. Mudstone (limestone) - MS\n",
        "6. Wackestone (limestone) - WS\n",
        "7. Dolomite - D\n",
        "8. Packstone-grainstone (limestone) - PS\n",
        "9. Phylloid-algal bafflestone (limestone) - BS\n",
        "\n",
        "These facies aren't discrete, and gradually blend into one another. Some have neighboring facies that are rather close.  Mislabeling within these neighboring facies can be expected to occur.  The following table lists the facies, their abbreviated labels and their approximate neighbors.\n",
        "\n",
        "Facies |Label| Adjacent Facies\n",
        ":---: | :---: |:--:\n",
        "1 |SS| 2\n",
        "2 |CSiS| 1,3\n",
        "3 |FSiS| 2\n",
        "4 |SiSh| 5\n",
        "5 |MS| 4,6\n",
        "6 |WS| 5,7\n",
        "7 |D| 6,8\n",
        "8 |PS| 6,7,9\n",
        "9 |BS| 7,8\n",
        "\n",
        "Let's clean up this dataset.  The 'Well Name' and 'Formation' columns can be turned into a categorical data type.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ANy2EJGzJ_ds"
      },
      "source": [
        "training_data['Well Name'] = training_data['Well Name'].astype('category')\n",
        "training_data['Formation'] = training_data['Formation'].astype('category')\n",
        "training_data['Well Name'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6PinlIEJ_dt"
      },
      "source": [
        "These are the names of the 10 training wells in the Council Grove reservoir.  Data has been recruited into pseudo-well 'Recruit F9' to better represent facies 9, the Phylloid-algal bafflestone. \n",
        "\n",
        "Before we plot the well data, let's define a color map so the facies are represented by consistent color in all the plots in this tutorial.  We also create the abbreviated facies labels, and add those to the `facies_vectors` dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr0l_42kJ_dt"
      },
      "source": [
        "# 1=sandstone  2=c_siltstone   3=f_siltstone \n",
        "# 4=marine_silt_shale 5=mudstone 6=wackestone 7=dolomite\n",
        "# 8=packstone 9=bafflestone\n",
        "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00',\n",
        "       '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
        "\n",
        "facies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS',\n",
        "                 'WS', 'D','PS', 'BS']\n",
        "\n",
        "\n",
        "#facies_color_map is a dictionary that maps facies labels\n",
        "#to their respective colors\n",
        "facies_color_map = dict(zip(facies_labels, facies_colors))\n",
        "\n",
        "## Include a column with the Facies labes                                        \n",
        "facies_labels_dic = dict(enumerate(facies_labels,start=1))\n",
        "\n",
        "training_data['FaciesLabels'] = training_data.Facies.map(facies_labels_dic)\n",
        "training_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpDn1BPFJ_du"
      },
      "source": [
        "Let's take a quick view of the statistical distribution of the input variables.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNEkkVIhJ_dv"
      },
      "source": [
        "training_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHjsFVC4J_dv"
      },
      "source": [
        "Looking at the `count` values, most values have 4149 valid values except for `PE`, which has 3232.  In this tutorial we will drop the feature vectors that don't have a valid `PE` entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDK4MUlqJ_dw"
      },
      "source": [
        "PE_mask = training_data['PE'].notnull().values\n",
        "training_data = training_data[PE_mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q47zh_mJ_dw"
      },
      "source": [
        "Let's take a look at the data from individual wells in a more familiar log plot form.  We will create plots for the five well log variables, as well as a log for facies labels.  The plots are based on the those described in [Alessandro Amato del Monte's tutorial](https://github.com/seg/tutorials/tree/master/1504_Seismic_petrophysics_1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "MfPEQFNwJ_dw"
      },
      "source": [
        "def make_facies_log_plot(logs, facies_colors):\n",
        "    n_facies  = len(facies_colors)\n",
        "    #make sure logs are sorted by depth\n",
        "    logs = logs.sort_values(by='Depth')\n",
        "    cmap_facies = colors.ListedColormap(\n",
        "            facies_colors[0:len(facies_colors)], 'indexed')\n",
        "\n",
        "    ztop=logs.Depth.min(); zbot=logs.Depth.max()\n",
        "    \n",
        "    cluster=np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)\n",
        "    \n",
        "    f, ax = plt.subplots(nrows=1, ncols=6, figsize=(8, 12))\n",
        "    ax[0].plot(logs.GR, logs.Depth, '-g')\n",
        "    ax[1].plot(logs.ILD_log10, logs.Depth, '-')\n",
        "    ax[2].plot(logs.DeltaPHI, logs.Depth, '-', color='0.5')\n",
        "    ax[3].plot(logs.PHIND, logs.Depth, '-', color='r')\n",
        "    ax[4].plot(logs.PE, logs.Depth, '-', color='black')\n",
        "    im=ax[5].imshow(cluster, interpolation='none', aspect='auto',\n",
        "                    cmap=cmap_facies,vmin=0.5,vmax=n_facies+.5)\n",
        "    divider = make_axes_locatable(ax[5])\n",
        "    cax = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n",
        "    cbar=plt.colorbar(im, cax=cax,ticks=np.arange(1,n_facies+1))\n",
        "    cbar.set_ticklabels(facies_labels)\n",
        "  \n",
        "    for i in range(len(ax)-1):\n",
        "        ax[i].set_ylim(ztop,zbot)\n",
        "        ax[i].invert_yaxis()\n",
        "        ax[i].grid()\n",
        "        ax[i].locator_params(axis='x', nbins=3)\n",
        "    \n",
        "    ax[0].set_xlabel(\"GR\")\n",
        "    ax[0].set_xlim(logs.GR.min(),logs.GR.max())\n",
        "    ax[1].set_xlabel(\"ILD_log10\")\n",
        "    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())\n",
        "    ax[2].set_xlabel(\"DeltaPHI\")\n",
        "    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())\n",
        "    ax[3].set_xlabel(\"PHIND\")\n",
        "    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())\n",
        "    ax[4].set_xlabel(\"PE\")\n",
        "    ax[4].set_xlim(logs.PE.min(),logs.PE.max())\n",
        "    ax[5].set_xlabel('Facies')\n",
        "    \n",
        "    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n",
        "    ax[4].set_yticklabels([]);\n",
        "    ax[5].set_yticklabels([]); ax[5].set_xticklabels([])\n",
        "    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih4v4xCMJ_dx"
      },
      "source": [
        "Placing the log plotting code in a function will make it easy to plot the logs from multiples wells, and can be reused later to view the results when we apply the facies classification model to other wells.  The function was written to take a list of colors and facies labels as parameters.  \n",
        "\n",
        "We then show log plots for well `SHRIMPLIN`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "wb0dxp60J_dx"
      },
      "source": [
        "make_facies_log_plot(\n",
        "    training_data[training_data['Well Name'] == 'SHRIMPLIN'],\n",
        "    facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWZUpYO2J_dx"
      },
      "source": [
        "and well `SHANKLE`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuKnIvI9J_dy"
      },
      "source": [
        "make_facies_log_plot(\n",
        "    training_data[training_data['Well Name'] == 'SHANKLE'],\n",
        "    facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_mmRNutJ_dy"
      },
      "source": [
        "In addition to individual wells, we can look at how the various facies are represented by the entire training set.  Let's plot a histogram of the number of training examples for each facies class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "6ZOQqj1gJ_dy"
      },
      "source": [
        "#count the number of unique entries for each facies, sort them by\n",
        "#facies number (instead of by number of entries)\n",
        "facies_counts = training_data['Facies'].value_counts().sort_index()\n",
        "\n",
        "#use facies labels to index each count\n",
        "facies_counts.index = facies_labels\n",
        "\n",
        "f, ax = plt.subplots(nrows=1, ncols=2, figsize = (20,10))\n",
        "facies_counts.plot(ax = ax[0], kind='bar',color=facies_colors, fontsize = 20,\n",
        "                   title='Distribution of Training Data by Facies')\n",
        "\n",
        "facies_counts.plot.pie(ax = ax[1], colors=facies_colors, fontsize = 20)\n",
        "facies_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIyacb3bJ_dz"
      },
      "source": [
        "facies_counts = training_data.groupby(['Facies', 'Well Name'])['Facies'].count().unstack()\n",
        "facies_counts.plot(kind='bar', stacked=True, figsize = (20,10),fontsize = 20)\n",
        "\n",
        "# Set Axis Properties\n",
        "plt.xticks(np.arange(9),labels=facies_labels,fontsize = 20)\n",
        "plt.yticks(fontsize = 15)\n",
        "plt.title('Distribution of Training Data by Facies Per Well', fontsize = 30)\n",
        "plt.legend(loc='best', prop={'size': 18})\n",
        "plt.xlabel('Facies', fontsize = 20)\n",
        "plt.ylabel('Numbers', fontsize = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRXrD0FiJ_dz"
      },
      "source": [
        "This shows the distribution of examples by facies for the 3232 training examples in the training set.  Dolomite (facies 7) has the fewest with 141 examples.  There are also only 185 bafflestone examples.  Depending on the performance of the classifier we are going to train, we may consider getting more examples of these facies.\n",
        "\n",
        "Crossplots are a familiar tool in the geosciences to visualize how two properties vary with rock type.  This dataset contains 5 log variables, and scatter matrix can help to quickly visualize the variation between the all the variables in the dataset.  We can employ the very useful [Seaborn library](https://stanford.edu/~mwaskom/software/seaborn/) to quickly create a nice looking scatter matrix. Each pane in the plot shows the relationship between two of the variables on the x and y axis, with each point is colored according to its facies.  The same colormap is used to represent the 9 facies.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "vdRFgvisJ_dz"
      },
      "source": [
        "#save plot display settings to change back to when done plotting with seaborn\n",
        "inline_rc = dict(mpl.rcParams)\n",
        "\n",
        "import seaborn as sns\n",
        "#sns.set()\n",
        "\n",
        "training_data_feaures = training_data.drop(['Well Name','Facies','Formation','Depth','NM_M','RELPOS'],axis=1)\n",
        "\n",
        "sns.pairplot(training_data_feaures, hue='FaciesLabels', palette=facies_color_map,  hue_order=reversed(facies_labels))\n",
        "\n",
        "#switch back to default matplotlib plot style\n",
        "mpl.rcParams.update(inline_rc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVh6a8sHJ_d0"
      },
      "source": [
        "Unfortunetaly, it is not clear from these crossplot what relationships exist between the measurements facies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_LFKdpsMZZQ"
      },
      "source": [
        "We can use heat maps to check on colinearitiy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTiTQRncMdrQ"
      },
      "source": [
        "fig=plt.figure(figsize=(17,8))\n",
        "sns.heatmap(training_data_feaures.corr(), cmap='coolwarm', annot=True, linewidths=4, linecolor='black')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8JZFK7UJ_d0"
      },
      "source": [
        "##  $\\color{red}{\\text{Implementation of Machine Learning Algorithms  }}$\n",
        "\n",
        "## Let's start with Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNlxvOYrJ_d0"
      },
      "source": [
        "### Conditioning the data set\n",
        "\n",
        "Now we extract just the feature variables we need to perform the classification.  The predictor variables are the five wireline values and two geologic constraining variables. We also get a vector of the facies labels that correspond to each feature vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZRWIJsvQKvF"
      },
      "source": [
        "correct_facies_labels = training_data['Facies'].values\n",
        "\n",
        "feature_vectors = training_data.drop(['Formation', 'Well Name', 'Depth','Facies','FaciesLabels'], axis=1)\n",
        "feature_vectors.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmxATmZvf6kn"
      },
      "source": [
        "###  Scaling the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsR9eIKYQYVg"
      },
      "source": [
        "Scikit includes a [preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html) module that can 'standardize' the data (giving each variable zero mean and unit variance, also called whitening). Many machine learning algorithms assume features will be standard normally distributed data (ie: Gaussian with zero mean and unit variance). The factors used to standardize the training set must be applied to any subsequent feature set that will be input to the classifier. The StandardScalar class can be fit to the training set, and later used to standardize any training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz_bCIobQmtW"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler().fit(feature_vectors)\n",
        "scaled_features = scaler.transform(feature_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf2_3UlmNZFE"
      },
      "source": [
        "scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1m1Wu-zNfw5"
      },
      "source": [
        "scaled_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1yfl8dmgDSo"
      },
      "source": [
        "### Splitting the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1_k4orSQspl"
      },
      "source": [
        "Scikit also includes a handy function to randomly split the training data into training and test sets.  The test set contains a small subset of feature vectors that are not used to train the network.  Because we know the true facies labels for these examples, we can compare the results of the classifier to the actual facies and determine the accuracy of the model.  Let's use 20% of the data for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4K_szPjQvEe"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_cv, y_train, y_cv = train_test_split(\n",
        "        scaled_features, correct_facies_labels, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhTi27jEJ_d2"
      },
      "source": [
        "### Training the Logistic Regression classifier\n",
        "\n",
        "Now we use the cleaned and conditioned training set to create a facies classifier.  As mentioned above, we will use a type of machine learning model known as a [Logistic Regression](LG).\n",
        "\n",
        "The LG implementation in [scikit-learn] (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) takes a number of important parameters.  First we create a classifier using the default settings.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "XL16pk_2J_d2"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "\n",
        "# We can look at all of the parameter setting for LR --> print out the LR object\n",
        "\n",
        "LogisticRegression() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egRj3uvTS7Fs"
      },
      "source": [
        "_Note_: One of the most important steps in using these classifiers is the right selection of what is called \"Hyperparameters\". For instance, \"C\" controls the intensity of the classifier in some ways - regularization parameter. This improves overfitting models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HTzhkI8Sn1D"
      },
      "source": [
        "lregression = LogisticRegression() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ytVPZq9TzQf"
      },
      "source": [
        "_Note_: we can see the inheritance of all other objects, attributes and methods associated with the object LogisticRegression   by simply typying \"dir\" --> this means you can use .fit or .predict with the regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7HUHqNoUHjY"
      },
      "source": [
        "dir(LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTohkDLnJ_d3"
      },
      "source": [
        "from time import timeNow we can train the classifier using the training set we created above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4hqG9FNPJ_d3"
      },
      "source": [
        "from time import time\n",
        "\n",
        "start_train  = time()\n",
        "lregression.fit(X_train,y_train)\n",
        "end_train = time()\n",
        "\n",
        "latency  =  round((end_train - start_train)*1000, 2)\n",
        "print(' Latency: {} ms'.format(latency))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nlrBrrJJ_d3"
      },
      "source": [
        "Now that the model has been trained on our data, we can use it to predict the facies of the feature vectors in the test set.  Because we know the true facies labels of the vectors in the test set, we can use the results to evaluate the accuracy of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "v9gie8ynJ_d3"
      },
      "source": [
        "start_pred  = time()\n",
        "y_pred_LR = lregression.predict(X_cv)\n",
        "\n",
        "end_pred= time()\n",
        "\n",
        "latency  =  round((end_pred - start_pred)*1000, 2)\n",
        "print(' Latency: {} ms'.format(latency))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qi3YCxjN_5J"
      },
      "source": [
        "y_pred_LR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUU2wAXJ_d4"
      },
      "source": [
        "We need some metrics to evaluate how good our classifier is doing.  A [confusion matrix](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) is a table that can be used to describe the performance of a classification model.  [Scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) allows us to easily create a confusion matrix by supplying the actual and predicted facies labels.\n",
        "\n",
        "The confusion matrix is simply a 2D array.  The entries of confusion matrix `C[i][j]` are equal to the number of observations predicted to have facies `j`, but are known to have facies `i`.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ZH-s-mZUJ_d4"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
        "                  (\"Normalized confusion matrix\", 'true')]\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize = (20,10))\n",
        "\n",
        "# for i, (a, b) in enumerate(zip(alist, blist)):\n",
        "for i, (title, normalize) in enumerate(titles_options):\n",
        "    plot_confusion_matrix(lregression, X_cv, y_cv, ax=ax[i],\n",
        "                                 display_labels=facies_labels,\n",
        "                                 normalize=normalize,\n",
        "                                 cmap='Blues')\n",
        "    ax[i].set_title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFEZ-nt2J_d4"
      },
      "source": [
        "The rows of the confusion matrix correspond to the actual facies labels.  The columns correspond to the labels assigned by the classifier.  For example, consider the first row. For the feature vectors in the test set that actually have label `SS`, 23 were correctly indentified as `SS`, 21 were classified as `CSiS` and 2 were classified as `FSiS`.\n",
        "\n",
        "The entries along the diagonal are the facies that have been correctly classified.  Below we define two functions that will give an overall value for how the algorithm is performing.  The accuracy is defined as the number of correct classifications divided by the total number of classifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1k8EI8iJ_d4"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['SS', 'CSiS', 'FSiS', 'SiSh','MS', 'WS', 'D','PS', 'BS']\n",
        "\n",
        "print(classification_report(y_cv, y_pred_LR, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "cPv73Yl0J_d5"
      },
      "source": [
        "def accuracy(conf):\n",
        "    total_correct = 0.\n",
        "    nb_classes = conf.shape[0]\n",
        "    for i in np.arange(0,nb_classes):\n",
        "        total_correct += conf[i,i]\n",
        "    acc = total_correct/sum(sum(conf))\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WPjVSEHJ_d5"
      },
      "source": [
        "As noted above, the boundaries between the facies classes are not all sharp, and some of them blend into one another.  The error within these 'adjacent facies' can also be calculated.  We define an array to represent the facies adjacent to each other.  For facies label `i`, `adjacent_facies[i]` is an array of the adjacent facies labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "MK2tfNh4J_d5"
      },
      "source": [
        "adjacent_facies = np.array([[1], [0,2], [1], [4], [3,5], [4,6,7], [5,7], [5,6,8], [6,7]])\n",
        "\n",
        "def accuracy_adjacent(conf, adjacent_facies):\n",
        "    nb_classes = conf.shape[0]\n",
        "    total_correct = 0.\n",
        "    for i in np.arange(0,nb_classes):\n",
        "        total_correct += conf[i,i]\n",
        "        for j in adjacent_facies[i]:\n",
        "            total_correct += conf[i,j]\n",
        "    return total_correct / sum(sum(conf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "sExVWWueJ_d5"
      },
      "source": [
        "conf = confusion_matrix(y_cv, y_pred_LR)\n",
        "print('Facies classification accuracy = %f' % accuracy(conf))\n",
        "print('Adjacent facies classification accuracy = %f' % accuracy_adjacent(conf, adjacent_facies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb7GDK0xclCq"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cwk17DgJ_d_"
      },
      "source": [
        "### Applying the classification model to new data\n",
        "\n",
        "Now that we have a trained facies classification model we can use it to identify facies in wells that do not have core data.  In this case, we will apply the classifier to two wells, but we could use it on any number of wells for which we have the same set of well logs for input.\n",
        "\n",
        "This dataset is similar to the training data except it does not have facies labels.  It is loaded into a dataframe called `test_data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "0-TKg5uTJ_eA"
      },
      "source": [
        "well_data = pd.read_csv('facies_test_data.csv')\n",
        "well_data['Well Name'] = well_data['Well Name'].astype('category')\n",
        "well_features = well_data.drop(['Formation', 'Well Name', 'Depth', 'Facies'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31yoqmz4J_eA"
      },
      "source": [
        "well_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06O5h4YiJ_eA"
      },
      "source": [
        "The data needs to be scaled using the same constants we used for the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B0vdEO4J_eA"
      },
      "source": [
        "X_test = scaler.transform(well_features)\n",
        "y_test = well_data['Facies'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N-EA4opJ_eB"
      },
      "source": [
        "Finally we predict facies labels for the unknown data, and store the results in a `Facies` column of the `test_data` dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1GYomLsDJ_eC"
      },
      "source": [
        "#predict facies of unclassified data\n",
        "# record time for prediction \n",
        "\n",
        "start_final  =time()  \n",
        "y_pred = lregression.predict(X_test)\n",
        "end_final= time()\n",
        "\n",
        "latency  =  round((end_final - start_final)*1000, 2)\n",
        "print(' Latency: {} ms'.format(latency))\n",
        "\n",
        "#print('Latency: {}ms'.format(latency)\n",
        "\n",
        "well_data['LR Prediction'] = y_pred\n",
        "well_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTlKjH-UVHry"
      },
      "source": [
        "Print latency time for prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFyl0_x9U6x8"
      },
      "source": [
        "print(' Latency: {} ms'.format(latency))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T33C5nP7J_eC"
      },
      "source": [
        "Let's see how we did with the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu2PXf_cJ_eC"
      },
      "source": [
        "cv_conf = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print('Optimized facies classification accuracy = %.2f' % accuracy(cv_conf))\n",
        "print('Optimized adjacent facies classification accuracy = %.2f' % accuracy_adjacent(cv_conf, adjacent_facies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "6Raty6pkJ_eD"
      },
      "source": [
        "well_data['Well Name'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJVM34SSJ_eD"
      },
      "source": [
        "We can use the well log plot to view the classification results along with the well logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0av3kz3FJ_eD"
      },
      "source": [
        "def compare_facies_plot(logs, prediction, facies_colors):\n",
        "    n_facies  = len(facies_colors)\n",
        "    #make sure logs are sorted by depth\n",
        "    logs = logs.sort_values(by='Depth')\n",
        "    cmap_facies = colors.ListedColormap(\n",
        "            facies_colors[0:len(facies_colors)], 'indexed')\n",
        "\n",
        "    ztop=logs.Depth.min(); zbot=logs.Depth.max()\n",
        "    \n",
        "    cluster1 = np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)\n",
        "    \n",
        "    cluster2 = np.repeat(np.expand_dims(logs[prediction].values,1), 100, 1)\n",
        "    \n",
        "    f, ax = plt.subplots(nrows=1, ncols=7, figsize=(9, 12))\n",
        "    ax[0].plot(logs.GR, logs.Depth, '-g')\n",
        "    ax[1].plot(logs.ILD_log10, logs.Depth, '-')\n",
        "    ax[2].plot(logs.DeltaPHI, logs.Depth, '-', color='0.5')\n",
        "    ax[3].plot(logs.PHIND, logs.Depth, '-', color='r')\n",
        "    ax[4].plot(logs.PE, logs.Depth, '-', color='black')\n",
        "    im1 = ax[5].imshow(cluster1, interpolation='none', aspect='auto',\n",
        "                    cmap=cmap_facies,vmin=0.5,vmax=n_facies+.5)\n",
        "    im2 = ax[6].imshow(cluster2, interpolation='none', aspect='auto',\n",
        "                     cmap=cmap_facies,vmin=0.5,vmax=n_facies+.5)\n",
        "    \n",
        "    divider = make_axes_locatable(ax[6])\n",
        "    cax  = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n",
        "    cbar = plt.colorbar(im2, cax=cax,ticks=np.arange(1,n_facies+1))\n",
        "    cbar.set_ticklabels(facies_labels)\n",
        "  \n",
        "    for i in range(len(ax)-2):\n",
        "        ax[i].set_ylim(ztop,zbot)\n",
        "        ax[i].invert_yaxis()\n",
        "        ax[i].grid()\n",
        "        ax[i].locator_params(axis='x', nbins=3)\n",
        "    \n",
        "    ax[0].set_xlabel(\"GR\")\n",
        "    ax[0].set_xlim(logs.GR.min(),logs.GR.max())\n",
        "    ax[1].set_xlabel(\"ILD_log10\")\n",
        "    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())\n",
        "    ax[2].set_xlabel(\"DeltaPHI\")\n",
        "    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())\n",
        "    ax[3].set_xlabel(\"PHIND\")\n",
        "    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())\n",
        "    ax[4].set_xlabel(\"PE\")\n",
        "    ax[4].set_xlim(logs.PE.min(),logs.PE.max())\n",
        "    ax[5].set_xlabel('Facies')\n",
        "    ax[6].set_xlabel(prediction)\n",
        "    \n",
        "    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n",
        "    ax[4].set_yticklabels([]);\n",
        "    ax[5].set_xticklabels([]); ax[5].set_xticklabels([])\n",
        "    ax[6].set_yticklabels([]); ax[6].set_xticklabels([])\n",
        "    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Rre9frakJ_eD"
      },
      "source": [
        "compare_facies_plot(\n",
        "    well_data[well_data['Well Name'] == 'STUART'],\n",
        "    'LR Prediction',\n",
        "    facies_colors=facies_colors)\n",
        "\n",
        "compare_facies_plot(\n",
        "    well_data[well_data['Well Name'] == 'CRAWFORD'],\n",
        "    'LR Prediction',\n",
        "    facies_colors=facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84M_l9gQuDLi"
      },
      "source": [
        "##  $\\color{red}{\\text{Logistic Regression - Hyperparameter Optimzation and K-Folding }}$\n",
        "\n",
        "## Let's start with Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S14S-7nuZ0A"
      },
      "source": [
        "We will use a fucntion in sklearn called GridSearchcv --> which makes an Exhaustive search (usign K-fold) over specified parameter values for an estimator. This means if you input a number of pre-defined hyperparameters. it will subsititute into our model object and perform K-fold. There are a lot of output functionality as we will see."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI8FZONUvYOC"
      },
      "source": [
        "## Create a print fucntion \n",
        "Now, I will create a function just to print some of the outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_kspzuduKZQ"
      },
      "source": [
        "def print_results(results):\n",
        "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
        "\n",
        "    means = results.cv_results_['mean_test_score']\n",
        "    stds = results.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
        "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWrustpAvVQw"
      },
      "source": [
        "## Perform OPT\n",
        "\n",
        "The classifier so far has been built with the default parameters. However, we may be able to get improved classification results with optimal parameter choices.\n",
        "\n",
        "We will consider one parameters. The parameter C is a regularization factor, and tells the classifier how much we want to avoid misclassifying training examples. A large value of C will try to correctly classify more examples from the training set, but if C is too large it may 'overfit' the data and fail to generalize when classifying new data. If C is too small then the model will not be good at fitting outliers and will have a large error on the training set.\n",
        "\n",
        "We will train a series of classifiers with different values for C. Two nested loops are used to train a classifier for every possible combination of values in the ranges specified. The classification accuracy is recorded for each combination of parameter values. The results are shown in a series of plots, so the parameter values that give the best classification accuracy on the test set can be selected.\n",
        "\n",
        "This process is also known as 'cross validation'. Often a separate 'cross validation' dataset will be created in addition to the training and test sets to do model selection. For this tutorial we will just use the test set to choose model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhJP2oLvyQ_B"
      },
      "source": [
        "### I do not want to mess up with my previous data, so I will copy to another variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOjdLJihxxCJ"
      },
      "source": [
        "X_train_OPT = scaled_features.copy()\n",
        "y_train_OPT = correct_facies_labels.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6nOnqQ0ICYU"
      },
      "source": [
        "We can initialize the K_fold by setting some parameters --> we can decide to use it or not! see below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XARJ-J-CHYar"
      },
      "source": [
        "# instantiate the Kfold CrossValidation\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "cv_strategy = KFold(n_splits=3, shuffle = True, random_state=125) # creates an instance\n",
        "cv_strategy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTWW_ccUvSp3"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {\n",
        "    'C': [0.0001, 0.01, 1, 5, 10, 20, 50, 100, 1000, 5000, 10000]\n",
        "}\n",
        "\n",
        "LR_cv = GridSearchCV(lregression , param_grid=parameters, cv=cv_strategy)\n",
        "#LR_cv = GridSearchCV(lregression , param_grid=parameters, cv=5)\n",
        "LR_cv.fit(X_train_OPT, y_train_OPT)\n",
        "\n",
        "print_results(LR_cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYDI4mc4vh_4"
      },
      "source": [
        "Check Best Estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huzDJsu7vgp_"
      },
      "source": [
        "LR_cv.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E8X1OhzRP6k"
      },
      "source": [
        "## *Write* out a pickled model --> Logistic Regression\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dab6941bRWo3"
      },
      "source": [
        "joblib.dump(LR_cv.best_estimator_, 'LR_model.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euv1Cvm5xGTZ"
      },
      "source": [
        "Display top 5 values of hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKxfMJanxFlf"
      },
      "source": [
        "LR_cv_results = pd.DataFrame(LR_cv.cv_results_)\n",
        "\n",
        "LR_cv_results.sort_values(by=['rank_test_score']).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM_gWfoWy-FR"
      },
      "source": [
        "## Let's predict the facies with the best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHcPIxHQzGjC"
      },
      "source": [
        "best_LR_clf = LogisticRegression(**LR_cv.best_params_)        \n",
        "best_LR_clf.fit(X_train,y_train)\n",
        "\n",
        "cv_conf = confusion_matrix(y_test, best_LR_clf.predict(X_test))\n",
        "\n",
        "print('Optimized facies classification accuracy = %.2f' % accuracy(cv_conf))\n",
        "print('Optimized adjacent facies classification accuracy = %.2f' % accuracy_adjacent(cv_conf, adjacent_facies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6AyK-xw1JLz"
      },
      "source": [
        "#predict facies of unclassified data\n",
        "y_LR_pred = best_LR_clf.predict(X_test)\n",
        "well_data['Best LR Prediction'] = y_LR_pred\n",
        "well_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkZqYfrF0goz"
      },
      "source": [
        "compare_facies_plot(\n",
        "    well_data[well_data['Well Name'] == 'STUART'],\n",
        "    'Best LR Prediction',\n",
        "    facies_colors=facies_colors)\n",
        "\n",
        "compare_facies_plot(\n",
        "    well_data[well_data['Well Name'] == 'CRAWFORD'],\n",
        "    'Best LR Prediction',\n",
        "    facies_colors=facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbgeEiTX2iH9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIhyJclL2cP-"
      },
      "source": [
        "##  $\\color{red}{\\text{Support Vecrtor Machine Implementation with  Hyperparameter Optimzation and K-Folding }}$\n",
        "\n",
        "## Let's start with Basic SVM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-WZ-6x82iWq"
      },
      "source": [
        "# Two ways to do it:\n",
        "#from sklearn import svm\n",
        "#svm_clf = svm.SVC()\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "SVC()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQFpBQKv4-W1"
      },
      "source": [
        "dir(SVC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1n61cIR430H"
      },
      "source": [
        "Now, we can just call the SVM function and fit the model as we have done with LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibzqf0u64AIU"
      },
      "source": [
        "svm_clf = SVC()\n",
        "\n",
        "svm_clf.fit(X_train,y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKxHc6av6bcv"
      },
      "source": [
        "y_pred_SVM = svm_clf.predict(X_cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwboZXgk6khF"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['SS', 'CSiS', 'FSiS', 'SiSh','MS', 'WS', 'D','PS', 'BS']\n",
        "\n",
        "print(classification_report(y_cv, y_pred_SVM, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6Fc3Hay6x9-"
      },
      "source": [
        "Optmization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59qKCEXCGUwR"
      },
      "source": [
        "# instantiate the Kfold CrossValidation\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "cv_strategy = KFold(n_splits=3, shuffle = True, random_state=125) # creates an instance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUEjN3h66VPH"
      },
      "source": [
        "parameters_SVM = {\n",
        "    #'kernel': ['linear', 'rbf'],\n",
        "    'C': [.01, 1, 5, 10, 20, 100],\n",
        "    'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "    #'C': [.01, 1, 5, 10, 20, 50, 100, 1000, 5000, 10000]\n",
        "}\n",
        "\n",
        "\n",
        "#SVM_cv = GridSearchCV(svm_clf, param_grid=parameters_SVM, cv=cv_strategy)\n",
        "SVM_cv = GridSearchCV(svm_clf, param_grid=parameters_SVM, cv=5)\n",
        "SVM_cv.fit(X_train_OPT, y_train_OPT)\n",
        "\n",
        "print_results(SVM_cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Zq1s8wFc3b"
      },
      "source": [
        "best_svm_clf = SVC(**SVM_cv.best_params_)      \n",
        "best_svm_clf.fit(X_train,y_train)\n",
        "\n",
        "cv_conf = confusion_matrix(y_cv, best_svm_clf.predict(X_cv))\n",
        "\n",
        "print('Optimized facies classification accuracy = %.2f' % accuracy(cv_conf))\n",
        "print('Optimized adjacent facies classification accuracy = %.2f' % accuracy_adjacent(cv_conf, adjacent_facies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evy0o9aZ7aqN"
      },
      "source": [
        "#predict facies of unclassified data\n",
        "y_pred_SVM_best = best_svm_clf.predict(X_test)\n",
        "well_data['Best SVM Prediction'] =y_pred_SVM_best\n",
        "well_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4U28jrn-bvX"
      },
      "source": [
        "compare_facies_plot(\n",
        "    well_data[well_data['Well Name'] == 'STUART'],\n",
        "    'Best SVM Prediction',\n",
        "    facies_colors=facies_colors)\n",
        "\n",
        "compare_facies_plot(\n",
        "    well_data[well_data['Well Name'] == 'CRAWFORD'],\n",
        "    'Best SVM Prediction',\n",
        "    facies_colors=facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs-A2mRmR80P"
      },
      "source": [
        "## *Write* out a pickled model --> SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC_IqLI0SAvY"
      },
      "source": [
        "joblib.dump(SVM_cv.best_estimator_, 'SVM_model.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlIzg0ndMWf-"
      },
      "source": [
        "##  $\\color{red}{\\text{Decision Trees and Random Forest Implementation with  Hyperparameter Optimzation and K-Folding }}$\n",
        "\n",
        "## Let's start with Basic Decision Tree (DT)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4dIzUy0Shji"
      },
      "source": [
        "from sklearn import tree                                                  # import decision tree from scikit-learn\n",
        "from sklearn.tree import DecisionTreeClassifier "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqxRbGJ7m1MZ"
      },
      "source": [
        "DecisionTree= DecisionTreeClassifier()\n",
        "DecisionTreeClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlA660H6hmNx"
      },
      "source": [
        "DT_clf = DecisionTreeClassifier()\n",
        "DT_clf.fit(X_train,y_train)\n",
        "# clf = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbHCHvLNZYvf"
      },
      "source": [
        "y_pred_DT = DT_clf.predict(X_cv)\n",
        "y_pred_DT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpM_CAlqtIDD"
      },
      "source": [
        "parameters_DT = {\n",
        "    'max_leaf_nodes': [3, 5, 10, 15, 20, 100],\n",
        "    'max_depth': [2, 4, 6, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "\n",
        "#DT_cv = GridSearchCV(DT_clf, param_grid=parameters_DT, cv=cv_strategy)\n",
        "DT_cv = GridSearchCV(DT_clf, param_grid=parameters_DT, cv=5)\n",
        "DT_cv.fit(X_train_OPT, y_train_OPT)\n",
        "\n",
        "print_results(DT_cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgLqlKsPbRum"
      },
      "source": [
        "#### Save optimal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlVVcOfXShph"
      },
      "source": [
        "best_DT_clf = DecisionTreeClassifier(**DT_cv.best_params_)      \n",
        "best_DT_clf.fit(X_train,y_train)\n",
        "\n",
        "cv_conf = confusion_matrix(y_cv, best_DT_clf.predict(X_cv))\n",
        "\n",
        "print('Optimized facies classification accuracy = %.2f' % accuracy(cv_conf))\n",
        "print('Optimized adjacent facies classification accuracy = %.2f' % accuracy_adjacent(cv_conf, adjacent_facies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEl-Ok9VShmY"
      },
      "source": [
        "#predict facies of unclassified data\n",
        "y_pred_DT_best = best_DT_clf.predict(X_test)\n",
        "well_data['Best DT Prediction'] =y_pred_DT_best\n",
        "well_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yyt66ypbzu2"
      },
      "source": [
        "compare_facies_plot(\n",
        "    well_data[well_data['Well Name'] == 'STUART'],\n",
        "    'Best DT Prediction',\n",
        "    facies_colors=facies_colors)\n",
        "\n",
        "compare_facies_plot(\n",
        "    well_data[well_data['Well Name'] == 'CRAWFORD'],\n",
        "    'Best DT Prediction',\n",
        "    facies_colors=facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPfcfiuGhjQK"
      },
      "source": [
        "## *Write* out a pickled model --> Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQwgJfR3ShsM"
      },
      "source": [
        "joblib.dump(DT_cv.best_estimator_, 'DT_model.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekwM1bBISc1M"
      },
      "source": [
        "## For the Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vtxTBZHOUyy"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rforest = RandomForestClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwoN5L7akK29"
      },
      "source": [
        " RandomForestClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCjecSCVOVmL"
      },
      "source": [
        "# perform cross-validation\n",
        "# dictionary with keys\n",
        "param_grid_strat = {'n_estimators': [10, 100, 200, 500],\n",
        "                   'min_samples_split': [2, 7, 15]}\n",
        "\n",
        "# what parameters to search depends on the expert knowledge\n",
        "\n",
        "RF_cv = GridSearchCV(rforest, param_grid=param_grid_strat, cv=cv_strategy)  # estimator\n",
        "\n",
        "RF_cv.fit(X_train, y_train)\n",
        "print(RF_cv.best_params_) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bOGw9RtOVsK"
      },
      "source": [
        "best_RF_clf = RandomForestClassifier(**RF_cv.best_params_)        \n",
        "best_RF_clf.fit(X_train,y_train)\n",
        "\n",
        "cv_conf = confusion_matrix(y_test, best_RF_clf.predict(X_test))\n",
        "\n",
        "print('Optimized facies classification accuracy = %.2f' % accuracy(cv_conf))\n",
        "print('Optimized adjacent facies classification accuracy = %.2f' % accuracy_adjacent(cv_conf, adjacent_facies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC81RT7OkzSO"
      },
      "source": [
        "#### Prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PGCs8drkxIu"
      },
      "source": [
        "#predict facies of unclassified data\n",
        "y_pred_RF_best = best_RF_clf.predict(X_test)\n",
        "well_data['Best RF Prediction'] =y_pred_RF_best\n",
        "well_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdBK_HLSkxMU"
      },
      "source": [
        "compare_facies_plot(\n",
        "    well_data[well_data['Well Name'] == 'STUART'],\n",
        "    'Best RF Prediction',\n",
        "    facies_colors=facies_colors)\n",
        "\n",
        "compare_facies_plot(\n",
        "    well_data[well_data['Well Name'] == 'CRAWFORD'],\n",
        "    'Best RF Prediction',\n",
        "    facies_colors=facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfSnkicLkxPN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNHjqD-xSolz"
      },
      "source": [
        "## *Write* out a pickled model --> Random Forest:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAvkDoi2h0VA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2_CpUAiOVx7"
      },
      "source": [
        "joblib.dump(RF_cv.best_estimator_, 'RF_model.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v67erKBMhQI6"
      },
      "source": [
        "## For Boosting (or Gradient Boosting using RF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcYoOeNnhprT"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e7fBW3Xhpt5"
      },
      "source": [
        "#clf = GradientBoostingClassifier(loss = 'deviance', max_depth=3, n_estimators = 100)\n",
        "#clf.fit(X_train, y_train)\n",
        "\n",
        "GradBoosting = GradientBoostingClassifier()\n",
        "GradientBoostingClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raQUIox4hpwZ"
      },
      "source": [
        "parameters_GB = {\n",
        "    'n_estimators': [5, 50, 250, 500],   #'n_estimators': [5, 50, 250, 500],\n",
        "    'max_depth': [1, 3, 5, 7, 9], # 'max_depth': [1, 3, 5, 7, 9],\n",
        "    'learning_rate': [0.01, 0.1, 1, 10, 100] # 'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "GB_cv = GridSearchCV(GradBoosting, param_grid=parameters_GB, cv=cv_strategy)  # estimator\n",
        "GB_cv.fit(X_train, y_train)\n",
        "print(RF_cv.best_params_) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xuScy44gfov"
      },
      "source": [
        "best_GB_clf = GradientBoostingClassifier(**GB_cv.best_params_)        \n",
        "best_GB_clf.fit(X_train,y_train)\n",
        "\n",
        "cv_conf = confusion_matrix(y_test, best_GB_clf.predict(X_test))\n",
        "\n",
        "print('Optimized facies classification accuracy = %.2f' % accuracy(cv_conf))\n",
        "print('Optimized adjacent facies classification accuracy = %.2f' % accuracy_adjacent(cv_conf, adjacent_facies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWEBt_4IhqR6"
      },
      "source": [
        "## *Write* out a pickled model --> Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFwXt2PEhpyq"
      },
      "source": [
        "joblib.dump(GB_cv.best_estimator_, 'GB_model.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL9gGJWjYzjg"
      },
      "source": [
        "## Other type of boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3drg3TC5Yy0K"
      },
      "source": [
        "#XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "XGBClassifier()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBAmiATOZjLv"
      },
      "source": [
        "# XGB = XGBClassifier(All te parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGlJk2jdZuxv"
      },
      "source": [
        "#ADABoost\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "AdaBoostClassifier()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOLnkyC-RNud"
      },
      "source": [
        "##  $\\color{red}{\\text{K-Nearest Neighboor (KNN)}\n",
        "}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8DfUzXoROOA"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "KNN_clf = KNeighborsClassifier()\n",
        "KNeighborsClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvM9n0fOROWR"
      },
      "source": [
        "# perform cross-validation\n",
        "# dictionary with keys\n",
        "param_grid_strat ={'n_neighbors': [5, 10, 50, 70],\n",
        "                   'algorithm': ['ball_tree', 'kd_tree', 'brute']} \n",
        "# what parameters to search depends on the expert knowledge\n",
        "\n",
        "KNN_cv  = GridSearchCV(KNN_clf, param_grid=param_grid_strat, cv=cv_strategy)  # estimator\n",
        "\n",
        "KNN_cv.fit(X_train, y_train)\n",
        "print(KNN_cv.best_params_) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkG4WnbxROY_"
      },
      "source": [
        "best_KNN_clf = KNeighborsClassifier(**KNN_cv.best_params_)        \n",
        "best_KNN_clf.fit(X_train,y_train)\n",
        "\n",
        "cv_conf = confusion_matrix(y_test, best_KNN_clf.predict(X_test))\n",
        "\n",
        "print('Optimized facies classification accuracy = %.2f' % accuracy(cv_conf))\n",
        "print('Optimized adjacent facies classification accuracy = %.2f' % accuracy_adjacent(cv_conf, adjacent_facies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sokVk_UZRObp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow2xCxBnROd9"
      },
      "source": [
        "joblib.dump(KNN_cv.best_estimator_, 'KNN_model.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bbRwhy0TNa4"
      },
      "source": [
        "##  $\\color{red}{\\text{Summary: Compare model results and final model selection}}$\n",
        "\n",
        "In this section, we will do the following:\n",
        "1. Evaluate all of our saved models on the validation set\n",
        "2. Select the best model based on performance on the validation set\n",
        "3. Evaluate that model on the holdout test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O89dD7tFdQih"
      },
      "source": [
        "### Supposed the dataset is in CSV files (if they are not, we will save it first is CSV so we can retrieve from disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M44lnZECUDZH"
      },
      "source": [
        "### Read in Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIY4Z5waTROr"
      },
      "source": [
        "models = {}\n",
        "\n",
        "#for mdl in ['LR', 'SVM', 'NN', 'RF', 'GB']:\n",
        "for mdl in ['LR', 'SVM', 'DT', 'RF', 'KNN', 'GB']:\n",
        "    models[mdl] = joblib.load('{}_model.pkl'.format(mdl))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVt2DRV_k74m"
      },
      "source": [
        "#Model = joblib.load('LR_model.pkl'.format('LR'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILxYNAkEULpc"
      },
      "source": [
        "models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-54pOo7MUbdk"
      },
      "source": [
        "models['LR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x8qCyaNbwUi"
      },
      "source": [
        "models.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTfydtqajf13"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWKCNvJhURGr"
      },
      "source": [
        "### Evaluate models on the validation set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8imtwiJHYB2z"
      },
      "source": [
        "#### Call all the metrics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9hh8f83YHdx"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xklNyMQVn21"
      },
      "source": [
        "round(accuracy_score(y_test, y_pred_SVM_best), 3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJLraJ-NdNOz"
      },
      "source": [
        "round(precision_score(y_test, y_pred_SVM_best,  average='weighted'), 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTuad4sleJU2"
      },
      "source": [
        "round(recall_score(y_test, y_pred_SVM_best , average='weighted'), 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG77HeLujaY3"
      },
      "source": [
        "round(f1_score(y_test, y_pred_SVM_best , average='weighted'), 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-kOkJp-UURm"
      },
      "source": [
        "def evaluate_model(name, model, features, labels):\n",
        "    start = time()\n",
        "    pred = model.predict(features)\n",
        "    end = time()\n",
        "    accuracy = round(accuracy_score(labels, pred), 3)\n",
        "    precision = round(precision_score(labels, pred, average='weighted'), 3)\n",
        "    recall = round(recall_score(labels, pred,  average='weighted'), 3)\n",
        "    f1 = round(f1_score(labels, pred,  average='weighted'), 3)\n",
        "    print('{} -- Accuracy: {} / Precision: {} / Recall: {} /f1: {} /  Latency: {}ms'.format(name,\n",
        "                                                                                   accuracy,\n",
        "                                                                                   precision,\n",
        "                                                                                   recall,\n",
        "                                                                                   f1,\n",
        "                                                                                   round((end - start)*1000, 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv487RJqUUUx"
      },
      "source": [
        "for name, mdl in models.items():\n",
        "    evaluate_model(name, mdl, X_cv, y_cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyK9KTCJUdOX"
      },
      "source": [
        "### Evaluate best model on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpZEZ1x8UUbT"
      },
      "source": [
        "evaluate_model('Random Forest', models['RF'], X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB5kWe7EJ_eH"
      },
      "source": [
        "## References\n",
        "\n",
        "Amato del Monte, A., 2015. Seismic Petrophysics: Part 1, *The Leading Edge*, 34 (4). [doi:10.1190/tle34040440.1](http://dx.doi.org/10.1190/tle34040440.1)\n",
        "\n",
        "Bohling, G. C., and M. K. Dubois, 2003. An Integrated Application of Neural Network and Markov Chain Techniques to Prediction of Lithofacies from Well Logs, *KGS Open-File Report* 2003-50, 6 pp. [pdf](http://www.kgs.ku.edu/PRS/publication/2003/ofr2003-50.pdf)\n",
        "\n",
        "Dubois, M. K., G. C. Bohling, and S. Chakrabarti, 2007, Comparison of four approaches to a rock facies classification problem, *Computers & Geosciences*, 33 (5), 599-617 pp. [doi:10.1016/j.cageo.2006.08.011](http://dx.doi.org/10.1016/j.cageo.2006.08.011)"
      ]
    }
  ]
}